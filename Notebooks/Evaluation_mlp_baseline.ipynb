{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31929,"status":"ok","timestamp":1683689242625,"user":{"displayName":"Neha Jain","userId":"09908231183836366208"},"user_tz":300},"id":"9iQfOPEL4gYd","outputId":"d7d393be-2fb2-4a51-d660-89caa31d6b1c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.11.0 (from transformers)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-frymm3v5\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-frymm3v5\n","  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ftfy (from clip==1.0)\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.65.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.1+cu118)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.27.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (8.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369370 sha256=7844bf9b1a6d8a44f9356e8d3520c1177a11f9bdd3c4b1c1f783b86aa4aef401\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-lparjd81/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n","Successfully built clip\n","Installing collected packages: ftfy, clip\n","Successfully installed clip-1.0 ftfy-6.1.1\n"]}],"source":["#@title Install\n","!pip install transformers\n","! pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16899,"status":"ok","timestamp":1683689259520,"user":{"displayName":"Neha Jain","userId":"09908231183836366208"},"user_tz":300},"id":"PzIdzzVY62_u","outputId":"507e0c9d-1689-4885-90cc-5e3fb3f337da"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\",force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cZZU19Ah67hT"},"outputs":[],"source":["import os\n","os.chdir(\"/content/gdrive/MyDrive/CS444_Neha/CS444Project/CLIP_prefix_caption/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fO-ycrHj4dLx"},"outputs":[],"source":["#@title Imports\n","\n","import clip\n","import os\n","from torch import nn\n","import numpy as np\n","import torch\n","import torch.nn.functional as nnf\n","import sys\n","from typing import Tuple, List, Union, Optional\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n","from tqdm import tqdm, trange\n","from google.colab import files\n","import skimage.io as io\n","import PIL.Image\n","from IPython.display import Image\n","\n","N = type(None)\n","V = np.array\n","ARRAY = np.ndarray\n","ARRAYS = Union[Tuple[ARRAY, ...], List[ARRAY]]\n","VS = Union[Tuple[V, ...], List[V]]\n","VN = Union[V, N]\n","VNS = Union[VS, N]\n","T = torch.Tensor\n","TS = Union[Tuple[T, ...], List[T]]\n","TN = Optional[T]\n","TNS = Union[Tuple[TN, ...], List[TN]]\n","TSN = Optional[TS]\n","TA = Union[T, ARRAY]\n","\n","\n","D = torch.device\n","CPU = torch.device('cpu')\n","\n","\n","def get_device(device_id: int) -> D:\n","    if not torch.cuda.is_available():\n","        return CPU\n","    device_id = min(torch.cuda.device_count() - 1, device_id)\n","    return torch.device(f'cuda:{device_id}')\n","\n","\n","CUDA = get_device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n62Y4gFZ4XY8"},"outputs":[],"source":["#@title Model\n","\n","class MLP(nn.Module):\n","\n","    def forward(self, x: T) -> T:\n","        return self.model(x)\n","\n","    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n","        super(MLP, self).__init__()\n","        layers = []\n","        for i in range(len(sizes) -1):\n","            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n","            if i < len(sizes) - 2:\n","                layers.append(act())\n","        self.model = nn.Sequential(*layers)\n","\n","\n","class ClipCaptionModel(nn.Module):\n","\n","    #@functools.lru_cache #FIXME\n","    def get_dummy_token(self, batch_size: int, device: D) -> T:\n","        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n","\n","    def forward(self, tokens: T, prefix: T, mask: Optional[T] = None, labels: Optional[T] = None):\n","        embedding_text = self.gpt.transformer.wte(tokens)\n","        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n","        #print(embedding_text.size()) #torch.Size([5, 67, 768])\n","        #print(prefix_projections.size()) #torch.Size([5, 1, 768])\n","        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n","        if labels is not None:\n","            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n","            labels = torch.cat((dummy_token, tokens), dim=1)\n","        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n","        return out\n","\n","    def __init__(self, prefix_length: int, prefix_size: int = 512):\n","        super(ClipCaptionModel, self).__init__()\n","        self.prefix_length = prefix_length\n","        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n","        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n","        if prefix_length > 10:  # not enough memory\n","            self.clip_project = nn.Linear(prefix_size, self.gpt_embedding_size * prefix_length)\n","        else:\n","            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2, self.gpt_embedding_size * prefix_length))\n","\n","\n","class ClipCaptionPrefix(ClipCaptionModel):\n","\n","    def parameters(self, recurse: bool = True):\n","        return self.clip_project.parameters()\n","\n","    def train(self, mode: bool = True):\n","        super(ClipCaptionPrefix, self).train(mode)\n","        self.gpt.eval()\n","        return self"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5S6Vccv387Os"},"outputs":[],"source":["#@title Caption prediction\n","\n","def generate_beam(model, tokenizer, beam_size: int = 5, prompt=None, embed=None,\n","                  entry_length=67, temperature=1., stop_token: str = '.'):\n","\n","    model.eval()\n","    stop_token_index = tokenizer.encode(stop_token)[0]\n","    tokens = None\n","    scores = None\n","    device = next(model.parameters()).device\n","    seq_lengths = torch.ones(beam_size, device=device)\n","    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n","    with torch.no_grad():\n","        if embed is not None:\n","            generated = embed\n","        else:\n","            if tokens is None:\n","                tokens = torch.tensor(tokenizer.encode(prompt))\n","                tokens = tokens.unsqueeze(0).to(device)\n","                generated = model.gpt.transformer.wte(tokens)\n","        for i in range(entry_length):\n","            outputs = model.gpt(inputs_embeds=generated)\n","            logits = outputs.logits\n","            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n","            logits = logits.softmax(-1).log()\n","            if scores is None:\n","                scores, next_tokens = logits.topk(beam_size, -1)\n","                generated = generated.expand(beam_size, *generated.shape[1:])\n","                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n","                if tokens is None:\n","                    tokens = next_tokens\n","                else:\n","                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n","                    tokens = torch.cat((tokens, next_tokens), dim=1)\n","            else:\n","                logits[is_stopped] = -float(np.inf)\n","                logits[is_stopped, 0] = 0\n","                scores_sum = scores[:, None] + logits\n","                seq_lengths[~is_stopped] += 1\n","                scores_sum_average = scores_sum / seq_lengths[:, None]\n","                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(beam_size, -1)\n","                next_tokens_source = next_tokens // scores_sum.shape[1]\n","                seq_lengths = seq_lengths[next_tokens_source]\n","                next_tokens = next_tokens % scores_sum.shape[1]\n","                next_tokens = next_tokens.unsqueeze(1)\n","                tokens = tokens[next_tokens_source]\n","                tokens = torch.cat((tokens, next_tokens), dim=1)\n","                generated = generated[next_tokens_source]\n","                scores = scores_sum_average * seq_lengths\n","                is_stopped = is_stopped[next_tokens_source]\n","            next_token_embed = model.gpt.transformer.wte(next_tokens.squeeze()).view(generated.shape[0], 1, -1)\n","            generated = torch.cat((generated, next_token_embed), dim=1)\n","            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n","            if is_stopped.all():\n","                break\n","    scores = scores / seq_lengths\n","    output_list = tokens.cpu().numpy()\n","    output_texts = [tokenizer.decode(output[:int(length)]) for output, length in zip(output_list, seq_lengths)]\n","    order = scores.argsort(descending=True)\n","    output_texts = [output_texts[i] for i in order]\n","    return output_texts\n","\n","\n","def generate2(\n","        model,\n","        tokenizer,\n","        tokens=None,\n","        prompt=None,\n","        embed=None,\n","        entry_count=1,\n","        entry_length=67,  # maximum number of words\n","        top_p=0.8,\n","        temperature=1.,\n","        stop_token: str = '.',\n","):\n","    model.eval()\n","    generated_num = 0\n","    generated_list = []\n","    stop_token_index = tokenizer.encode(stop_token)[0]\n","    filter_value = -float(\"Inf\")\n","    device = next(model.parameters()).device\n","\n","    with torch.no_grad():\n","\n","        for entry_idx in range(entry_count):\n","            if embed is not None:\n","                generated = embed\n","            else:\n","                if tokens is None:\n","                    tokens = torch.tensor(tokenizer.encode(prompt))\n","                    tokens = tokens.unsqueeze(0).to(device)\n","\n","                generated = model.gpt.transformer.wte(tokens)\n","\n","            for i in range(entry_length):\n","\n","                outputs = model.gpt(inputs_embeds=generated)\n","                logits = outputs.logits\n","                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n","                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n","                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n","                sorted_indices_to_remove = cumulative_probs > top_p\n","                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n","                                                    ..., :-1\n","                                                    ].clone()\n","                sorted_indices_to_remove[..., 0] = 0\n","\n","                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n","                logits[:, indices_to_remove] = filter_value\n","                next_token = torch.argmax(logits, -1).unsqueeze(0)\n","                next_token_embed = model.gpt.transformer.wte(next_token)\n","                if tokens is None:\n","                    tokens = next_token\n","                else:\n","                    tokens = torch.cat((tokens, next_token), dim=1)\n","                generated = torch.cat((generated, next_token_embed), dim=1)\n","                if stop_token_index == next_token.item():\n","                    break\n","\n","            output_list = list(tokens.squeeze().cpu().numpy())\n","            output_text = tokenizer.decode(output_list)\n","            generated_list.append(output_text)\n","\n","    return generated_list[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6VOqclm8bUsn"},"outputs":[],"source":["# Validation Data \n","import zipfile\n","\n","val_data_zip = zipfile.ZipFile(\"/content/gdrive/MyDrive/CS444_Neha/CS444Project/CLIP_prefix_caption/data/coco/val2014.zip\",\"r\") #Opens the tar file in read mode\n","val_data_zip.extractall(\"/tmp\") #Extracts the files into the /tmp folder\n","val_data_zip.close()"]},{"cell_type":"code","source":["import json\n","from pycocotools.coco import COCO\n","from pycocoevalcap.eval import COCOEvalCap\n","#from clipcap_model import ClipCapModel # import your ClipCap model\n","\n","# Load the COCO annotations\n","annFile = '/content/gdrive/MyDrive/CS444_Neha/CS444Project/CLIP_prefix_caption/data/coco/annotations/captions_val2014.json' # change this to the path of the annotations file\n","coco = COCO(annFile)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6HK6Ok7vdJUj","executionInfo":{"status":"ok","timestamp":1683689353260,"user_tz":300,"elapsed":6144,"user":{"displayName":"Neha Jain","userId":"09908231183836366208"}},"outputId":"f34b1069-d808-4de8-c781-47e3d3ba43a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loading annotations into memory...\n","Done (t=0.77s)\n","creating index...\n","index created!\n"]}]},{"cell_type":"code","source":["import random\n","# Select a random subset of image IDs\n","imgIds = [image_id for image_id in coco.getImgIds()]\n","imgIds = list(set(imgIds))  # remove duplicates\n","# Set the seed\n","random.seed(123)\n","\n","random.shuffle(imgIds,)\n","imgIds = imgIds[:5000]"],"metadata":{"id":"joQo6XJ2c9lx"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":99,"referenced_widgets":["87405f917af54c26813f8ab7700c2a66","75b045a978ea44e2bcffa096537553b6","3d9dc024cbf2489a95c7cfed1d58cd15","da2963f6d3aa4e6f96d7199b5b0a5cf2","df80e45ce13746d587e4df5be1102b54","48b98f2308fc4c25a8fbc4c38ef4a408","769aca47327b478b9d02d59b93f7c962","c13843c99727412593a036641d77a664","ccd1b835a14d4713a4991208e15e22bb","f7952d6e58a54f9594456633e600e59b","f08b3f212bbb4de9ae90dbf2bbec8a43","5a4811a7bfa845818fabdc7208508079","a858b08f50bc4a67b211f26e1347b31f","e0385c86cb824f38a3873da730dad850","eaccf5ffff8647f7be8b148a55845ef6","bf2e8bd15f554634a7876885ebbd4a98","2b74e3c5ac0143fc85d63307bf045024","1931c16ccd9949bbbb5ac8131fc0bc4d","5413529457c540d3988c17abe1687c98","dd4ad19645a845bd9022a154e4dd89c4","28c7d6ae0eaf4838a2cde87e332173cc","6c7fe187d16f47dea6d454180e384d75"]},"id":"3kXG5yBi38ST","outputId":"c5eb7c48-f270-4c4a-8678-7d83af4210db","executionInfo":{"status":"ok","timestamp":1683690364600,"user_tz":300,"elapsed":983233,"user":{"displayName":"Neha Jain","userId":"09908231183836366208"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|████████████████████████████████████████| 338M/338M [00:03<00:00, 115MiB/s]\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87405f917af54c26813f8ab7700c2a66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a4811a7bfa845818fabdc7208508079"}},"metadata":{}}],"source":["# Load the ClipCap model\n","current_directory = os.getcwd()\n","save_path =  \"/content/gdrive/MyDrive/CS444_Neha/CS444Project/pretrained_models\"\n","model_path = os.path.join(save_path, 'mlp_gpt2_weights.pt')\n","\n","prefix_length = 10\n","\n","model = ClipCaptionModel(prefix_length)\n","\n","model.load_state_dict(torch.load(model_path, map_location=CPU)) \n","\n","model = model.eval() \n","is_gpu = True  \n","device = CUDA(0) if is_gpu else \"cpu\"\n","model = model.to(device)# clipCap model \n","\n","#@title CLIP model + GPT2 tokenizer\n","\n","device = CUDA(0) if is_gpu else \"cpu\"\n","clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","\n","use_beam_search = False \n","prefix_length = 10\n","\n","current_directory = os.getcwd()\n","\n","# Generate captions for the test set\n","results = []\n","images_path = \"/tmp/val2014/\"\n","for img_id in imgIds:\n","    img = coco.loadImgs(img_id)[0]\n","    name_ = f\"COCO_val2014_{int(img_id):012d}.jpg\"\n","    UPLOADED_FILE = os.path.join(images_path, name_)\n","    if not os.path.isfile(UPLOADED_FILE):\n","      continue\n","\n","    image = io.imread(UPLOADED_FILE)\n","    pil_image = PIL.Image.fromarray(image)\n","    image = preprocess(pil_image).unsqueeze(0).to(device)\n","    with torch.no_grad():\n","        prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n","        prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n","    if use_beam_search:\n","        generated_text_prefix = generate_beam(model, tokenizer, embed=prefix_embed)[0]\n","    else:\n","        generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)\n","\n","    #caption = clipcap.generate_caption(file_name) # generate a caption using your ClipCap model\n","    results.append({\n","        'image_id': img_id,\n","        'caption': generated_text_prefix\n","    })\n","\n","# Evaluate the results using the COCO evaluation metrics\n","resFile = 'mlp_baseline_results.json' # save the results to a JSON file\n","with open(resFile, 'w') as f:\n","    json.dump(results, f)\n"]},{"cell_type":"code","source":["cocoRes = coco.loadRes(resFile) # load the generated captions\n","cocoEval = COCOEvalCap(coco, cocoRes)\n","cocoEval.evaluate()\n","\n","# Print the evaluation results\n","for metric, score in cocoEval.eval.items():\n","    print('%s: %.4f' % (metric, score))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VEKSI0ercSwX","executionInfo":{"status":"ok","timestamp":1683690400707,"user_tz":300,"elapsed":36129,"user":{"displayName":"Neha Jain","userId":"09908231183836366208"}},"outputId":"088d4267-d79e-48fb-fe0d-d9dcb822b5a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading and preparing results...\n","DONE (t=0.03s)\n","creating index...\n","index created!\n","tokenization...\n","setting up scorers...\n","computing Bleu score...\n","{'testlen': 49206, 'reflen': 48617, 'guess': [49206, 44206, 39206, 34206], 'correct': [38636, 22723, 11985, 6193]}\n","ratio: 1.0121151037702654\n","Bleu_1: 0.785\n","Bleu_2: 0.635\n","Bleu_3: 0.498\n","Bleu_4: 0.387\n","computing METEOR score...\n","METEOR: 0.299\n","computing Rouge score...\n","ROUGE_L: 0.593\n","computing CIDEr score...\n","CIDEr: 1.307\n","Bleu_1: 0.7852\n","Bleu_2: 0.6353\n","Bleu_3: 0.4978\n","Bleu_4: 0.3866\n","METEOR: 0.2987\n","ROUGE_L: 0.5927\n","CIDEr: 1.3068\n"]}]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"87405f917af54c26813f8ab7700c2a66":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_75b045a978ea44e2bcffa096537553b6","IPY_MODEL_3d9dc024cbf2489a95c7cfed1d58cd15","IPY_MODEL_da2963f6d3aa4e6f96d7199b5b0a5cf2"],"layout":"IPY_MODEL_df80e45ce13746d587e4df5be1102b54"}},"75b045a978ea44e2bcffa096537553b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48b98f2308fc4c25a8fbc4c38ef4a408","placeholder":"​","style":"IPY_MODEL_769aca47327b478b9d02d59b93f7c962","value":"Downloading (…)olve/main/vocab.json: 100%"}},"3d9dc024cbf2489a95c7cfed1d58cd15":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c13843c99727412593a036641d77a664","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ccd1b835a14d4713a4991208e15e22bb","value":1042301}},"da2963f6d3aa4e6f96d7199b5b0a5cf2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7952d6e58a54f9594456633e600e59b","placeholder":"​","style":"IPY_MODEL_f08b3f212bbb4de9ae90dbf2bbec8a43","value":" 1.04M/1.04M [00:00&lt;00:00, 34.3MB/s]"}},"df80e45ce13746d587e4df5be1102b54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48b98f2308fc4c25a8fbc4c38ef4a408":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"769aca47327b478b9d02d59b93f7c962":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c13843c99727412593a036641d77a664":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccd1b835a14d4713a4991208e15e22bb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7952d6e58a54f9594456633e600e59b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f08b3f212bbb4de9ae90dbf2bbec8a43":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a4811a7bfa845818fabdc7208508079":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a858b08f50bc4a67b211f26e1347b31f","IPY_MODEL_e0385c86cb824f38a3873da730dad850","IPY_MODEL_eaccf5ffff8647f7be8b148a55845ef6"],"layout":"IPY_MODEL_bf2e8bd15f554634a7876885ebbd4a98"}},"a858b08f50bc4a67b211f26e1347b31f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b74e3c5ac0143fc85d63307bf045024","placeholder":"​","style":"IPY_MODEL_1931c16ccd9949bbbb5ac8131fc0bc4d","value":"Downloading (…)olve/main/merges.txt: 100%"}},"e0385c86cb824f38a3873da730dad850":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5413529457c540d3988c17abe1687c98","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dd4ad19645a845bd9022a154e4dd89c4","value":456318}},"eaccf5ffff8647f7be8b148a55845ef6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28c7d6ae0eaf4838a2cde87e332173cc","placeholder":"​","style":"IPY_MODEL_6c7fe187d16f47dea6d454180e384d75","value":" 456k/456k [00:00&lt;00:00, 29.7MB/s]"}},"bf2e8bd15f554634a7876885ebbd4a98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b74e3c5ac0143fc85d63307bf045024":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1931c16ccd9949bbbb5ac8131fc0bc4d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5413529457c540d3988c17abe1687c98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd4ad19645a845bd9022a154e4dd89c4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"28c7d6ae0eaf4838a2cde87e332173cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c7fe187d16f47dea6d454180e384d75":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}